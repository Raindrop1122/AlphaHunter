[
    {
        "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos",
        "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.",
        "authors": [
            "Seungjae Lee",
            "Yoonkyo Jung",
            "Inkook Chun",
            "Yao-Chih Lee",
            "Zikui Cai",
            "Hongjia Huang",
            "Aayush Talreja",
            "Tan Dat Dao",
            "Yongyuan Liang",
            "Jia-Bin Huang",
            "Furong Huang"
        ],
        "pdf_url": "https://arxiv.org/pdf/2511.21690v1",
        "published": "2025-11-26",
        "ai_score": 2,
        "ai_verdict": "论文聚焦机器人学习，与量化交易关联弱，实战价值有限。",
        "ai_strategy": "风控"
    },
    {
        "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
        "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
        "authors": [
            "Hongjin Su",
            "Shizhe Diao",
            "Ximing Lu",
            "Mingjie Liu",
            "Jiacheng Xu",
            "Xin Dong",
            "Yonggan Fu",
            "Peter Belcak",
            "Hanrong Ye",
            "Hongxu Yin",
            "Yi Dong",
            "Evelina Bakhturina",
            "Tao Yu",
            "Yejin Choi",
            "Jan Kautz",
            "Pavlo Molchanov"
        ],
        "pdf_url": "https://arxiv.org/pdf/2511.21689v1",
        "published": "2025-11-26",
        "ai_score": 8,
        "ai_verdict": "高效轻量级模型协调工具，提升量化决策效率与成本优势。",
        "ai_strategy": "多因子"
    },
    {
        "title": "Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework",
        "summary": "Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \\textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\\times$ higher data generation throughput under identical hardware resources, without compromising output quality.",
        "authors": [
            "Dong Wang",
            "Yang Li",
            "Ansong Ni",
            "Ching-Feng Yeh",
            "Youssef Emad",
            "Xinjie Lei",
            "Liam Robbins",
            "Karthik Padthe",
            "Hu Xu",
            "Xian Li",
            "Asli Celikyilmaz",
            "Ramya Raghavendra",
            "Lifei Huang",
            "Carole-Jean Wu",
            "Shang-Wen Li"
        ],
        "pdf_url": "https://arxiv.org/pdf/2511.21686v1",
        "published": "2025-11-26",
        "ai_score": 7,
        "ai_verdict": "框架提升数据生成效率，但需验证合成数据在量化中的适用性。",
        "ai_strategy": "多因子"
    },
    {
        "title": "Agentic Learner with Grow-and-Refine Multimodal Semantic Memory",
        "summary": "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.",
        "authors": [
            "Weihao Bo",
            "Shan Zhang",
            "Yanpeng Sun",
            "Jingjing Wu",
            "Qunyi Xie",
            "Xiao Tan",
            "Kunbin Chen",
            "Wei He",
            "Xiaofan Li",
            "Na Zhao",
            "Jingdong Wang",
            "Zechao Li"
        ],
        "pdf_url": "https://arxiv.org/pdf/2511.21678v1",
        "published": "2025-11-26",
        "ai_score": 7,
        "ai_verdict": "提升多模态推理，减少错误重复，对复杂策略有潜在价值。",
        "ai_strategy": "多因子"
    },
    {
        "title": "On Evolution-Based Models for Experimentation Under Interference",
        "summary": "Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.",
        "authors": [
            "Sadegh Shirani",
            "Mohsen Bayati"
        ],
        "pdf_url": "https://arxiv.org/pdf/2511.21675v1",
        "published": "2025-11-26",
        "ai_score": 8,
        "ai_verdict": "网络因果效应新方法，实战中可优化多因子策略，但需警惕时间趋势干扰。",
        "ai_strategy": "多因子"
    }
]