import arxiv
import json
import time
import os
import random
import requests
import sys
from datetime import datetime

# --- é…ç½®åŒºåŸŸ ---
API_KEY = os.environ.get("DEEPSEEK_API_KEY")

if not API_KEY:
    print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ã€‚")
    sys.exit(1)

DB_FILE = 'papers.json'
SEARCH_QUERY = 'cat:q-fin.ST OR cat:q-fin.PM OR cat:cs.LG OR cat:q-fin.TR OR cat:q-fin.CP'

def get_deepseek_analysis(paper):
    url = "https://api.deepseek.com/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }

    # æ ¸å¿ƒä¿®æ”¹ï¼šåŠ å¼ºç‰ˆ Promptï¼Œå¼ºåˆ¶åˆ†ç‚¹ï¼Œå¼ºåˆ¶åŒè¯­ç‹¬ç«‹
    system_prompt = """
    You are an elite Wall Street Quant Researcher (Alpha Hunter).
    Your task is to analyze academic papers for a hedge fund.
    
    CRITICAL FORMATTING RULES:
    1. Output MUST be valid JSON.
    2. 'summary_cn' and 'verdict_cn' MUST be in CHINESE.
    3. 'summary_en' and 'verdict_en' MUST be in ENGLISH.
    4. For summaries and verdicts, use bullet points starting with 'â€¢'.
    5. Each section MUST have at least 3 distinct bullet points.
    6. Be critical, concise, and professional.
    """

    user_prompt = f"""
    Analyze this paper:
    Title: {paper.title}
    Abstract: {paper.summary}

    Return JSON with this EXACT structure:
    {{
        "ai_score": (float 0-10, strict evaluation, <6 is trash),
        "translated_title": (Translate title to simplified Chinese),
        
        "summary_en": (3-5 bullet points in English. Focus on: â€¢ Model Architecture â€¢ Data used â€¢ Performance metrics.),
        "summary_cn": (3-5ä¸ªä¸­æ–‡åˆ†ç‚¹ã€‚æ ¼å¼ï¼šâ€¢ æ ¸å¿ƒæ¨¡å‹: ... â€¢ æ•°æ®æ¥æº: ... â€¢ ä¸»è¦ç»“è®º: ...),
        
        "verdict_en": (3-5 bullet points in English. Focus on: â€¢ Alpha Potential â€¢ Implementation Risk â€¢ Novelty.),
        "verdict_cn": (3-5ä¸ªä¸­æ–‡åˆ†ç‚¹ã€‚çŠ€åˆ©ç‚¹è¯„ï¼šâ€¢ åˆ›æ–°ç‚¹: ... â€¢ å®ç›˜å‘: ... â€¢ å¤ç°éš¾åº¦: ...),
        
        "ai_strategy": (Select ONE: "High-Freq", "Arbitrage", "Alpha-Factor", "Risk-Mgmt", "Crypto", "NLP/LLM", "Deep-Learning"),
        
        "journal_info": {{
            "name": (Predict venue e.g., 'J.Finance', 'NeurIPS', 'ICML' or 'ArXiv Preprint'),
            "status": (Predict: 'Preprint', 'Under Review', 'Accepted')
        }}
    }}
    """

    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "response_format": { "type": "json_object" },
        "temperature": 0.3
    }

    try:
        response = requests.post(url, headers=headers, json=payload, timeout=45) # å¢åŠ è¶…æ—¶æ—¶é—´
        if response.status_code != 200:
             print(f"âš ï¸ API Error: {response.status_code} - {response.text}")
             return None
        return json.loads(response.json()['choices'][0]['message']['content'])
    except Exception as e:
        print(f"âŒ Connection Failed: {e}")
        return None

def main():
    print(f"ğŸš€ Alpha Hunter Scraper Started at {datetime.now()}")
    
    existing_papers = []
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, 'r', encoding='utf-8') as f:
                existing_papers = json.load(f)
        except:
            existing_papers = []
    
    existing_ids = {p['pdf_url'] for p in existing_papers}
    print(f"ğŸ“š Loaded {len(existing_papers)} existing papers.")

    client = arxiv.Client()
    search = arxiv.Search(
        query=SEARCH_QUERY,
        max_results=10, # æ¯æ¬¡æ›´æ–°10ç¯‡ï¼Œé¿å…APIè¶…æ—¶
        sort_by=arxiv.SortCriterion.SubmittedDate
    )

    new_entries = []
    
    try:
        results = list(client.results(search))
    except Exception as e:
        print(f"âŒ ArXiv Failed: {e}")
        return

    for result in results:
        if result.pdf_url in existing_ids:
            continue
            
        print(f"ğŸ” Analyzing: {result.title[:50]}...")
        analysis = get_deepseek_analysis(result)
        
        if not analysis:
            print("   -> Skipped (API Error)")
            continue

        paper_entry = {
            "id": result.pdf_url.split('/')[-1],
            "title": result.title,
            "pdf_url": result.pdf_url,
            "published": result.published.strftime("%Y-%m-%d"),
            "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "ai_score": analysis.get("ai_score", 0),
            "translated_title": analysis.get("translated_title", result.title),
            "summary_en": analysis.get("summary_en", "â€¢ Analysis pending."),
            "summary_cn": analysis.get("summary_cn", "â€¢ åˆ†æå¤±è´¥æˆ–ç­‰å¾…ä¸­ã€‚"),
            "verdict_en": analysis.get("verdict_en", "â€¢ No verdict."),
            "verdict_cn": analysis.get("verdict_cn", "â€¢ æš‚æ— é”è¯„ã€‚"),
            "ai_strategy": analysis.get("ai_strategy", "Other"),
            "journal_info": analysis.get("journal_info", {"name": "ArXiv", "status": "Preprint"})
        }
        
        new_entries.append(paper_entry)
        print(f"âœ… Indexed! Score: {paper_entry['ai_score']}")
        time.sleep(2) 

    if new_entries:
        updated_db = new_entries + existing_papers
        updated_db = updated_db[:1500] # é™åˆ¶æ€»åº“å¤§å°
        with open(DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(updated_db, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Saved {len(new_entries)} papers.")
    else:
        print("ğŸ’¤ No new papers.")

if __name__ == "__main__":
    main()
