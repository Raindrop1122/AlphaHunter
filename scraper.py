import arxiv
import json
import time
import os
import random
import requests
import sys
from datetime import datetime

# --- é…ç½®åŒºåŸŸ ---
# âš ï¸ GitHub ç‰ˆæœ¬å…³é”®ä¿®æ”¹ï¼š
# ç§»é™¤ç¡¬ç¼–ç çš„ Keyï¼Œå¼ºåˆ¶ä»ç¯å¢ƒå˜é‡è·å–ã€‚
# åœ¨ GitHub Actions ä¸­ï¼Œè¿™ä¼šè‡ªåŠ¨è¯»å–ä½ è®¾ç½®çš„ Secretsã€‚
API_KEY = os.environ.get("DEEPSEEK_API_KEY")

# å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ²¡æœ‰è·å–åˆ° Keyï¼Œç›´æ¥ç»ˆæ­¢ç¨‹åº
if not API_KEY:
    print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ã€‚")
    print("ğŸ’¡ æç¤ºï¼š")
    print("   1. æœ¬åœ°è¿è¡Œï¼šè¯·åœ¨ç»ˆç«¯è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œæˆ–åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ã€‚")
    print("   2. GitHub Actionsï¼šè¯·ç¡®ä¿åœ¨ Settings > Secrets and variables > Actions ä¸­æ·»åŠ äº† 'DEEPSEEK_API_KEY'ã€‚")
    sys.exit(1)

DB_FILE = 'papers.json'

# ArXiv æœç´¢å…³é”®è¯ (è¦†ç›–é‡‘èå·¥ç¨‹ã€æœºå™¨å­¦ä¹ ã€åŠ å¯†è´§å¸)
SEARCH_QUERY = 'cat:q-fin.ST OR cat:q-fin.PM OR cat:cs.LG OR cat:q-fin.TR OR cat:q-fin.CP'

def get_deepseek_analysis(paper):
    """è°ƒç”¨ DeepSeek API è¿›è¡Œæ·±åº¦åˆ†æ (æ¯’èˆŒ + æç‚¼)"""
    
    url = "https://api.deepseek.com/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }

    # System Prompt: è®¾å®šäººè®¾å’Œä¸¥æ ¼çš„æ ¼å¼è¦æ±‚
    system_prompt = """
    You are an elite Wall Street Quant Researcher. 
    Your job is to filter ArXiv papers for a hedge fund trading desk.
    
    CRITICAL OUTPUT RULES:
    1. Output MUST be valid JSON.
    2. 'summary_cn' and 'verdict_cn' MUST be in simplified Chinese.
    3. 'summary_en' and 'verdict_en' MUST be in English.
    4. Keep content CONCISE and BULLISH/BEARISH. No academic fluff.
    """

    # User Prompt: å…·ä½“æŒ‡ä»¤
    user_prompt = f"""
    Analyze this paper:
    Title: {paper.title}
    Abstract: {paper.summary}

    Return JSON with this EXACT structure:
    {{
        "ai_score": (float 0-10, be strict, <6 is trash),
        "translated_title": (Chinese translation),
        
        "summary_en": (Format as 3-5 bullet points using 'â€¢ '. Focus on: 1. Core Model/Algorithm 2. Data Source 3. Key Findings. Keep it short.),
        "summary_cn": (æ ¼å¼ä¸º3-5ä¸ª'â€¢ 'å¼€å¤´çš„åˆ†ç‚¹ã€‚é‡ç‚¹æç‚¼ï¼š1. æ ¸å¿ƒæ¨¡å‹/ç®—æ³• 2. æ•°æ®æ¥æº 3. ä¸»è¦ç»“è®ºã€‚æ‹’ç»åºŸè¯ï¼Œç›´å‡»è¦å®³ã€‚),
        
        "verdict_en": (3 bullet points using 'â€¢ '. 1. Innovation 2. Real-world Trading Risk 3. Implementation Difficulty),
        "verdict_cn": (3ä¸ª'â€¢ 'å¼€å¤´çš„åˆ†ç‚¹æ¯’èˆŒç‚¹è¯„ã€‚1. åˆ›æ–°ç‚¹åœ¨å“ªé‡Œ 2. å®ç›˜ä¼šæœ‰ä»€ä¹ˆå‘ 3. å¤ç°éš¾æ˜“åº¦),
        
        "ai_strategy": (Select ONE: "High-Freq", "Arbitrage", "Alpha-Factor", "Risk-Mgmt", "Crypto", "NLP/LLM", "Deep-Learning"),
        
        "journal_info": {{
            "name": (Predict target venue e.g. 'J.Finance', 'NeurIPS', 'ICML' or 'ArXiv Garbage'),
            "status": (Predict: 'Preprint', 'Under Review', 'Accepted')
        }}
    }}
    """

    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "response_format": { "type": "json_object" },
        "temperature": 0.3
    }

    try:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        # æ‰“å°çŠ¶æ€ç ä»¥ä¾¿è°ƒè¯•
        if response.status_code != 200:
             print(f"âš ï¸ API Error: {response.status_code} - {response.text}")
        response.raise_for_status()
        return json.loads(response.json()['choices'][0]['message']['content'])
    except Exception as e:
        print(f"âŒ DeepSeek Connection Failed: {e}")
        return None

def main():
    print(f"ğŸš€ Alpha Hunter Scraper Started at {datetime.now()}")
    
    # 1. è¯»å–ç°æœ‰æ•°æ®åº“
    existing_papers = []
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, 'r', encoding='utf-8') as f:
                existing_papers = json.load(f)
        except:
            existing_papers = []
    
    existing_ids = {p['pdf_url'] for p in existing_papers}
    print(f"ğŸ“š Loaded {len(existing_papers)} existing papers.")

    # 2. çˆ¬å– ArXiv
    print("ğŸ“¡ Fetching from ArXiv...")
    client = arxiv.Client()
    search = arxiv.Search(
        query=SEARCH_QUERY,
        max_results=10,  # è¿™é‡Œçš„æ•°é‡å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
        sort_by=arxiv.SortCriterion.SubmittedDate
    )

    new_entries = []
    
    try:
        results = list(client.results(search))
    except Exception as e:
        print(f"âŒ ArXiv Connection Failed: {e}")
        # åœ¨ GitHub Actions ä¸­ï¼Œå¦‚æœè¿ä¸ä¸Š ArXivï¼Œå¯èƒ½éœ€è¦é‡è¯•æˆ–ç›´æ¥å¤±è´¥
        return

    for result in results:
        # è·³è¿‡å·²å­˜åœ¨çš„
        if result.pdf_url in existing_ids:
            continue
            
        print(f"ğŸ” Analyzing: {result.title[:50]}...")
        
        # è°ƒç”¨ AI
        analysis = get_deepseek_analysis(result)
        
        # å¦‚æœåˆ†æå¤±è´¥ï¼ˆå¯èƒ½å› ä¸ºç½‘ç»œæˆ–é…é¢ï¼‰ï¼Œä¸ºäº†ä¸ä¸­æ–­æµç¨‹ï¼Œå¯ä»¥é€‰æ‹©è·³è¿‡æˆ–å­˜ä¸€ä¸ªç©ºè®°å½•
        # è¿™é‡Œé€‰æ‹©è·³è¿‡
        if not analysis:
            print("   -> Analysis skipped due to API error.")
            continue

        # ç»„è£…æ•°æ®
        paper_entry = {
            "id": result.pdf_url.split('/')[-1], # ä½¿ç”¨ ArXiv ID
            "title": result.title,
            "pdf_url": result.pdf_url,
            "published": result.published.strftime("%Y-%m-%d"),
            "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            
            # AI å­—æ®µ
            "ai_score": analysis.get("ai_score", 0),
            "translated_title": analysis.get("translated_title", result.title),
            "summary_en": analysis.get("summary_en", "Analysis failed."),
            "summary_cn": analysis.get("summary_cn", "åˆ†æå¤±è´¥ã€‚"),
            "verdict_en": analysis.get("verdict_en", "No verdict."),
            "verdict_cn": analysis.get("verdict_cn", "æš‚æ— ç‚¹è¯„ã€‚"),
            "ai_strategy": analysis.get("ai_strategy", "Other"),
            "journal_info": analysis.get("journal_info", {"name": "ArXiv", "status": "Preprint"})
        }
        
        new_entries.append(paper_entry)
        print(f"âœ… Indexed! Score: {paper_entry['ai_score']}")
        
        # âš ï¸ é‡è¦ï¼šé¿å…é¢‘ç¹è¯·æ±‚è§¦å‘é€Ÿç‡é™åˆ¶ï¼ŒDeepSeek ä¹Ÿæœ‰ QPS é™åˆ¶
        time.sleep(2) 

    # 3. ä¿å­˜æ›´æ–°
    if new_entries:
        # æ–°è®ºæ–‡æ”¾å‰é¢
        updated_db = new_entries + existing_papers
        # ä¿æŒæ•°æ®åº“ä¸è¿‡å¤§ï¼Œåªå­˜æœ€è¿‘ 2000 ç¯‡
        updated_db = updated_db[:2000]
        
        with open(DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(updated_db, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Saved {len(new_entries)} new papers. Total: {len(updated_db)}")
    else:
        print("ğŸ’¤ No new papers found or all analysis failed.")

if __name__ == "__main__":
    main()
